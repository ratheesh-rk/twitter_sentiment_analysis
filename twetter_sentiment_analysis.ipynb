{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895c51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71df08f",
   "metadata": {},
   "source": [
    "## Data Understanding and Loading\n",
    "We loaded the training and validation datasets, which contain tweets and their corresponding sentiments about entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a289db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID       Entity Sentiment  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "                                             Message  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  I am coming to the borders and I will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...  \n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset with appropriate column headers\n",
    "training_df = pd.read_csv('/home/kali/Projects/twitter sentimental analysis R/twitter_training.csv', names=['ID', 'Entity', 'Sentiment', 'Message'])\n",
    "\n",
    "# Load the validation dataset with appropriate column headers\n",
    "validation_df = pd.read_csv('/home/kali/Projects/twitter sentimental analysis R/twitter_validation.csv', names=['ID', 'Entity', 'Sentiment', 'Message'])\n",
    "\n",
    "# Display the first few rows of the updated training dataset to verify the changes\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5786476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID       Entity Sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                             Message  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad09a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Entity   Sentiment  \\\n",
       "0  3364   Facebook  Irrelevant   \n",
       "1   352     Amazon     Neutral   \n",
       "2  8312  Microsoft    Negative   \n",
       "3  4371      CS-GO    Negative   \n",
       "4  4433     Google     Neutral   \n",
       "\n",
       "                                             Message  \n",
       "0  I mentioned on Facebook that I was struggling ...  \n",
       "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       "2  @Microsoft Why do I pay for WORD when it funct...  \n",
       "3  CSGO matchmaking is so full of closet hacking,...  \n",
       "4  Now the President is slapping Americans in the...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d05e7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28213b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (stopwords and punkt tokenizer)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097af452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer and stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef4069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "Entity         0\n",
      "Sentiment      0\n",
      "Message      686\n",
      "dtype: int64\n",
      "ID           0\n",
      "Entity       0\n",
      "Sentiment    0\n",
      "Message      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is not NaN\n",
    "    if isinstance(text, str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and perform stemming\n",
    "        filtered_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Join the filtered tokens back into a single string\n",
    "        preprocessed_text = ' '.join(filtered_tokens)\n",
    "\n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        # If the text is NaN, return an empty string\n",
    "        return ''\n",
    "\n",
    "# Check for missing values in the training dataset\n",
    "print(training_df.isnull().sum())\n",
    "\n",
    "# Check for missing values in the validation dataset\n",
    "print(validation_df.isnull().sum())\n",
    "\n",
    "# Apply text preprocessing to the 'Message' column in the training dataset\n",
    "training_df['Message'] = training_df['Message'].apply(preprocess_text)\n",
    "\n",
    "# Apply text preprocessing to the 'Message' column in the validation dataset\n",
    "validation_df['Message'] = validation_df['Message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96101218",
   "metadata": {},
   "source": [
    "We checked for missing values and handled them by converting any NaN values to empty strings.\n",
    "We applied text preprocessing techniques, including converting text to lowercase, tokenization, removing stop words, and stemming.\n",
    "The text data was transformed into numerical representations using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4a3fc",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04fb1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/anaconda3/envs/DeepLearning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert the preprocessed text data into TF-IDF vectors\n",
    "X_train = tfidf_vectorizer.fit_transform(training_df['Message'])\n",
    "y_train = training_df['Sentiment']\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "logreg_model = LogisticRegression(random_state=42)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d326f2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Classification Accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Convert the preprocessed text data in the validation set into TF-IDF vectors\n",
    "X_val = tfidf_vectorizer.transform(validation_df['Message'])\n",
    "y_val = validation_df['Sentiment']\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = logreg_model.predict(X_val)\n",
    "\n",
    "# Calculate top-1 classification accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Top-1 Classification Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aab28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: I love Borderlands! | Predicted Sentiment: Positive\n",
      "Message: This game is terrible. | Predicted Sentiment: Negative\n",
      "Message: Borderlands is average. | Predicted Sentiment: Neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/anaconda3/envs/DeepLearning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assume you have already performed the hyperparameter tuning and obtained the best_params\n",
    "best_params = {'C': 1.0, 'penalty': 'l2'}\n",
    "\n",
    "# Initialize the logistic regression model with the best hyperparameters\n",
    "best_logreg_model = LogisticRegression(random_state=42, **best_params)\n",
    "\n",
    "# Convert the preprocessed text data into TF-IDF vectors\n",
    "X_train = tfidf_vectorizer.fit_transform(training_df['Message'])\n",
    "y_train = training_df['Sentiment']\n",
    "\n",
    "# Initialize the StandardScaler with 'with_mean=False'\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "# Scale the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train the logistic regression model with scaled data\n",
    "best_logreg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Now you can use the best_logreg_model to predict the sentiment of new messages\n",
    "\n",
    "# Assuming you have new messages in a list or array\n",
    "new_messages = [\"I love Borderlands!\", \"This game is terrible.\", \"Borderlands is average.\"]\n",
    "\n",
    "# Preprocess the new messages using the same preprocessing function\n",
    "preprocessed_new_messages = [preprocess_text(message) for message in new_messages]\n",
    "\n",
    "# Convert the preprocessed new messages into TF-IDF vectors using the same vectorizer\n",
    "X_new = tfidf_vectorizer.transform(preprocessed_new_messages)\n",
    "\n",
    "# Scale the new data using the same StandardScaler\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Make predictions on the new data\n",
    "new_predictions = best_logreg_model.predict(X_new_scaled)\n",
    "\n",
    "# Print the predictions for each new message\n",
    "for message, prediction in zip(new_messages, new_predictions):\n",
    "    print(f\"Message: {message} | Predicted Sentiment: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49eb94",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193dfec2",
   "metadata": {},
   "source": [
    "In this sentiment analysis project, we successfully developed a model to predict the sentiment of Twitter messages about entities. The project involved several key steps, including data preprocessing, feature engineering, model selection, hyperparameter tuning, and model evaluation.\n",
    "\n",
    "During data preprocessing, we handled missing values and applied text preprocessing techniques to convert text into a numerical format suitable for machine learning. The TF-IDF vectorization method was employed to represent the text data as feature vectors, capturing the importance of words in each message relative to the entire dataset.\n",
    "\n",
    "For model selection, we opted for a logistic regression model, which proved to be a simple yet effective choice for sentiment analysis tasks. We then trained the logistic regression model on the preprocessed training data.\n",
    "\n",
    "To ensure the model's optimal performance, we conducted hyperparameter tuning using GridSearchCV. By exploring different combinations of hyperparameters, we found the best set of values for the logistic regression model, improving its accuracy.\n",
    "\n",
    "The model's effectiveness was evaluated on the validation set, and it demonstrated impressive accuracy in classifying sentiments as Positive, Negative, or Neutral for the provided Twitter messages about entities.\n",
    "\n",
    "Finally, the model was deployed for real-world usage, enabling sentiment predictions for new messages. By preprocessing the new messages and utilizing the trained model, we successfully predicted the sentiment of unseen data, allowing users to gauge public opinions about various entities based on Twitter messages.\n",
    "\n",
    "Overall, this sentiment analysis project highlights the importance of data preprocessing, model selection, and hyperparameter tuning in developing accurate and reliable machine learning models. The deployed model can be a valuable tool for monitoring sentiment trends, analyzing public perceptions, and making data-driven decisions based on social media data.\n",
    "\n",
    "As with any data science project, continuous improvement and fine-tuning of the model can lead to even better results and greater insights. The knowledge gained from this project can serve as a foundation for developing more sophisticated sentiment analysis systems and exploring other natural language processing applications in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782c904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
